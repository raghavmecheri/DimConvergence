<html>
   <head>
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/reveal.min.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/css/theme/white.min.css">
   </head>
   <body>
      <div class="reveal">
         <div class="slides">
            <section>
               <section>
                  <h3>Convergence of Dimensionality Reduction</h3>
                  Raghav Mecheri & Ketan Jog
               </section>
               <section>
                  <b>Project Goal:</b> Perform an Empirical Analysis of UMAP, to study properties like convergence and embedding quality
               </section>
            </section>
            <section>
               <section>
                  <b>Overview</b>: Dimensionality Reduction & UMAP in particular
               </section>
               <section>
                  Dimensionality Reduction
               </section>
               <section>
                  <h4>t-Sne: A Recap</h4>
                  <ul>
                     <li>
                        Used primarily for visualisation (since it ignores global structure!)
                     </li>
                     <li>
                        Attempts to preserve <b>only</b> local geometry -- a focus on preserving the similarity between points in local neighbourhoods
                     </li>
                     <li>
                        Underlying concept: Model the original and embedded space neighbourhoods as a probability distributions, and then find a low dimensional embedding that reduces the KL-divergence between them. 
                     </li>
                     <li>
                        Non deterministic (if you run it multiple times, you don't always get the same output unless you lock a seed)
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>t-SNE: The Issues</h4>
                  <ul>
                     <li>
                       It doesn't learn an explicit function to map new points (note: this can be worked around by treating this as an optimisation problem, but there's no f(x) that serves as a mapping)
                     </li>
                     <li>
                        t-SNE has many hyper-parameters that need to be defined empirically (dataset-specific)
                     </li>
                     <li>
                        It does not preserve global structure (only local)
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>UMAP: An Overview</h4>
                  <ul>
                     <li>
                        Can be used for both visualisation, but also for <b>general</b> dimensionality reduction
                     </li>
                     <li>
                        It preserves both local, and global structure (to an extent)
                     </li>
                     <li>
                        Any distance metric can be used with UMAP
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>UMAP: Assumptions and Concept</h4>
                  <ul>
                     <li>
                        Data lies on a manifold
                     </li>
                     <li>
                        This manifold is locally connected (no holes)
                     </li>
                     <li>
                        Construct a <b>topological</b> representation of the data in the ambient and embedded space
                     </li>
                     <li>
                        Minimise cross-entropy across topological structures
                     </li>
                  </ul>
               </section>
               <section>
                  <h4> k-Simplex: convex hull of k+1 points</h4>>
                  <img src="https://umap-learn.readthedocs.io/en/latest/_images/simplices.png">
               </section>
               <section>
                  <h4>Simple Cover</h4>
                  <img src="https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_open_cover.png">
               </section>
               <section>
                  <h4>Unconnected Manifold</h4>
                  <img src="https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_basic_graph.png">
               </section>
               <section>
                  <h4>Vary metric in each neighborhood</h4>
                  <img src="https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_local_metric_open_cover.png">
               </section>
               <section>
                  <h4>Take fuzzy union across balls</h4>
                  <img src="https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_fuzzy_open_cover.png">
               </section>
               <section>
                  <h4>End up with a graph representation</h4>
                  <img src="https://umap-learn.readthedocs.io/en/latest/_images/how_umap_works_umap_graph.png">
               </section>
               <section>
                  <li>Using this graph as our original space representation, we optimise for the embedding space representation using cross entropy.</li>>
                  <img src=cross.png alt='cross'>
               </section>
               <section>
                  <h4>UMAP: Summary</h4>
                  <ul>
                     <li>
                        Efficient, non linear dimensionality reduction. O(n) complexity
                     </li>
                     <li>
                        It can use any distance metric -- isn't restricted to Euclidian space!
                     </li>
                     <li>
                        Not completely stochastic like t-SNE
                     </li>
                     <li>
                        Defines both local and global structure, <b>and</b> creates a deterministic bijection for a mapping
                     </li>
                  </ul>
               </section>
            </section>
            <section>
               <section>
                  <h4>Our Experiment (Original)</h4>
                  Can we use UMAP's ability to hot-swap metrics in order to understand how the stability and convergence of dimensionality reduction varies with both the size of the dataset, and the input/output metrics?
               </section>
               <section>
                  <h4>Our Experiment (Modified)</h4>
                  Can we study the behavior and performance of UMAP and t-SNE as the size of our dataset scales?
               </section>
               <section>
                  <h4>Algorithms considered</h4>
                  <ul>
                     <li>
                        UMAP
                     </li>
                     <li>
                        t-SNE
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Datasets considered</h4>
                  <ul>
                     <li>
                        MNIST
                     </li>
                     <li>
                        Fashion MNIST
                     </li>
                     <li>
                        Olivetti faces
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Experiments</h4>
                  <ol>
                     <li>
                        Understanding how the spread, furthest-nearest neighbour comparision, and epsilon-ball precision-recall curves vary across various sizes of these datasets, for both random and stratified sampling, and across algorithms
                     </li>
                     <li>
                        Understanding the variation of inter-point distance as n scales for two of these datasets (MNIST, Olivetti faces) for both random and stratified sampling, and across algorithms
                     </li>
                  </ol>
               </section>
               <section>
                  <h4>Experimental Notes</h4>
                  <ul>
                     <li>10 runs of every experimental combination were recorded, in order to remove potential variance</li>
                     <li>Different random seeds were fixed per experimental run, in order to keep experimental results constant within runs</li>
                  </ul>
               </section>
               <section>
                  <ul>
                     <li>A fixed holdout set was considered for every run of our second experiment, in order to evaluate the inter-point distance of new data in an existing embedding</li>
                     <li>Modified versions of the <a href="https://umap-learn.readthedocs.io/en/latest/">umap-learn</a> and <a href="https://opentsne.readthedocs.io/en/latest/">openTSNE</a>, an accelerated t-SNE package, were used</li>
                     <li>Embedding new points into an existing t-SNE embedding was treated as an optimisation problem, more on this <a href="https://opentsne.readthedocs.io/en/latest/">here</a></li>
                  </ul>
               </section>
            </section>
            <section>
               <section>
                  <h4>Experimental Inferences</h4>
                  Here were our initial impressions
               </section>
               <section>
                  <li>To understand how an embedding of data in R^2 is affected as the amount of data increases</li>
                  <li>Does a UMAP / tSNE representation "stabilise"?</li>
               </section>
               <section>
                  <li> We used KNN and FNN precision scores to measure how UMAP preserved the structure of the data </li>
                  <li> If local structure is well-preserved, then the NN in the original data space should match the NN in the embedded space <li>
               </section>
               <section>
                  <img src="knnfnn.png" alt="KNN-FNN">
               </section>
               <section>
                  Does the precision wrt similarity measures change as we increase the data?
               </section>
               <section>
                  <img src="umapepsilon.png" alt="umap-epsilon">
               </section>
               <section>
                  Is this just the effect of working with more data? Well...
               </section>
               <section>
                  <img src="tsneepsilon.png" alt="tsne-epsilon">
               </section>
               <section>
                  Lets look at whether UMAP (or tSNE) converge to a single representation...what could measure this?
               </section>
               <section>
                  <li>We chose the LDA score (ie intercluster distance V intracluster distance)</li>
                  <li> Turns out that scatter converges to a value on some dataset-algorithm pairs..</li>
               </section>
               <section>
                  <img src="scat_mnist_stable.png" alt="scat-mnist-stable">
               </section>
               <section>
                  <img src="scat_olivetti_stable.png" alt="scat-olivetti-stable">
               </section>
               <section>
                  Other times, it doesn't......
               </section>
               <section>
                  <img src="scat_ unstable.png" alt="scat-unstable">
               </section>
               <section>
                  <li> This needs further exploration</li>
                  <li> Does it mean there is a "sufficient" amount of data that allows us to render a stable embedding of the distribution into the dataset?</li>
               </section>
               <section>
                  <li>We now look at the "embedded space". How do we measure the difference between 2 embeddings</li>
                  <li> Interpoint Distance: Look at the L2 deviation of an embedded datapoint from one embedding to the other</li>
               </section>
               <section>
                  We look at the deviation of Avg Interpoint Distance of the "true" embedding from that generated from less data.
               </section>
               <section>
                  <img src="truth_stable.png" alt="truth-stable">
               </section>
               <section>
                  <img src="conv_true.png" alt="conv-true">
               </section>
               <section>
                  <img src="conv_increment.png" alt="conv-increment">
               </section>
               <section>
                  <li>UMAP needs a much smaller set of data to narrow down on an embedding </li>
                  <li>The method of sampling didnâ€™t seem to have much of an effect on the performance </li>
               </section>
               <section>
                  <li>UMAP requires a much smaller dataset than tSNE to rightly model an embedded space. On the other hand, we could also say tSNE improves the embedding as we scale with data</li>
                  <li> Test by measuring clustering performance</li>
               </section>
            </section>
            <section>
               <section>
                  <h4>Future Work</h4>
                  <ul>
                     <li>
                        Studying the impact of n on convergence on data of different configurations
                     </li>
                     <li>
                        Understanding the impact of various (input, output) metric pairs on UMAP's performance as the size of the dataset scales
                     </li>
                     <li>
                        Studying the potential issues with measurement of performance of dimensionality reduction
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Studying various data configurations</h4>
                  <ul>
                     <li>
                        Can we understand which data configurations scale better with size?
                     </li>
                     <li>
                        Our current experiments show us a trend -- certain datasets seem to scale better with size, but we could be mistaken here
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Studying different (input, output) metric pairs</h4>
                  <ul>
                     <li>
                        UMAP allows us to hot-swap input-output metric pairs while performing dimensionality reduction
                     </li>
                     <li>
                        We think it would be cool to think about how the input-output metric pair could potentially influence how well we can reduce the dimensionality of data, both dependant and independant of n
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Understanding convergence across dimensions</h4>
                  <ul>
                     <li>
                        A lot of the techniques we've used depend on dimensionality -- are we messing up by assuming that metrics like points covered by an epsilon ball are even valid
                     </li>
                     <li>
                        The curse of dimensionality is definitely playing a part here. But how much?
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Clustering Performance on UMAP</h4>
                  <ul>
                     <li>
                        Use UMAP as a preprocessing step, and see how clustering improves as n grows...stability?
                     </li>
                     <li>
                        Might help understand if the theres a "sufficient" amount of data that models a data distribution well.
                     </li>
                  </ul>
               </section>
               <section>
                  <h4>Semi-supervised UMAP</h4>
                  <ul>
                     <li>
                        Can weak labelling or labelling a small subset accelerate "convergence"
                     </li>
                     <li>
                        How much does labelling affect performance (value of UMAP as an unsupervised method)
                     </li>
                  </ul>
               </section>
            </section>
         </div>
      </div>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.6.0/js/reveal.js"></script>
      <script>
         Reveal.initialize({
           mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
           config: 'TeX-AMS_HTML-full',
           TeX: { Macros: { RR: "{\\bf R}" } }
         });
      </script>
   </body>
</html>